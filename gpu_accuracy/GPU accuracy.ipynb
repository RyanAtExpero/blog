{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Quick Note on GPU Accuracy and Double Precision\n",
    "\n",
    "Often when optimizing simulation software, we run into a classic roadblock:  *Floating point precision is not accurate enough, we must use double precision.*\n",
    "\n",
    "Of course, there are many instances in real physical problems where indeed, a solution requires double precision computation.  However, we should be careful not to fall into a trap - the performance benefits of using 32bit vs 64bit datatypes is **significant**\n",
    "\n",
    "Particularly on gpus, where memory and bandwidth charge a higher premium, 32bit operations are much more desireable (see also Machine Learning's recent push to use 16 bit operations).\n",
    "\n",
    "But we do have one fascinating edge on our side when using GPU calculations: they are often more accurate than CPU calculations.\n",
    "\n",
    "No, I don't mean to say that the GPU somehow has a more accurate FPU.  Instead, due to the massively number of GPU threads and correspondingly small units of work, GPU threads more often deal with numbers of comparable sizes and are thus more accurate.\n",
    "\n",
    "## Example: Sum of a double precision array.\n",
    "\n",
    "Take as a simple example reducing the sum of an array.  A classic CPU loop will look something like:\n",
    "\n",
    "    sum = 0\n",
    "    for element in array:\n",
    "      sum += element\n",
    "    return sum\n",
    "\n",
    "As the sum gets larger, so also does the error.\n",
    "\n",
    "For a GPU, we will often use a tree based approach:\n",
    "<img src=\"tree.png\" width=\"300\">\n",
    "\n",
    "Due to the tree like nature, the elements sent to the FPU tend to have equivalent sizes and the sum operation therefore has less error.\n",
    "\n",
    "Lets look at a simple examply using *numpy* and *pycuda*   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pycuda.autoinit\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.gpuarray as gpuarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "\n",
    "#source double precision array\n",
    "d = np.random.rand(N) \n",
    "\n",
    "#floating point CPU copy\n",
    "f = d.astype(np.float32)\n",
    "\n",
    "#floating point GPU copy\n",
    "g = gpuarray.to_gpu(f)\n",
    "\n",
    "#sums\n",
    "dsum = d.sum()\n",
    "fsum = f.sum()\n",
    "gsum = gpuarray.sum(g).get()\n",
    "\n",
    "#distance from double\n",
    "fd = np.abs(dsum-fsum)\n",
    "gd = np.abs(dsum-gsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.plot([0, 0],[0, fd], c='blue', label='cpu distance: %f' %(fd))\n",
    "plt.plot([0, gd],[0, 0], c='green', label='gpu distance: %f' %(gd))\n",
    "ax = plt.axes()\n",
    "ax.set_xbound(-0.2 * fd, 1.1 * fd)\n",
    "ax.set_ybound(-0.2 * fd, 1.1 * fd)\n",
    "plt.legend()\n",
    "\n",
    "print(\"True sum:\", dsum)\n",
    "print(\"CPU 32bit precision:\", fsum)\n",
    "print(\"GPU 32bit precision:\", gsum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And there you have it.  The GPU 32bit sum is quite a bit closer to the true double precision sum than CPU 32bit sum.\n",
    "\n",
    "Here we have a simple example with a simple reduction, but in reality there are many more examples where we can relax our double precision requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dsum,fsum,gsum)\n",
    "print(fsum-49947.5, gsum-49947.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
